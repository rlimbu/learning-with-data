{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction With PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pricipal component analysis (PCA) is a technique from linear algebra that is used to reduce the number of dimensions or attributes of datasets without losing too much of their explanatory power.\n",
    "\n",
    "Why reduce dimensionality? Because it is (a)computationally less expensive to process datasets with fewer dimensions and (b) easier to visualize datasets with fewer dimensions. \n",
    "\n",
    "Ultimately, the goal of PCA is to alleviate the so-called curse of dimensionality.\n",
    "\n",
    "The scikit-learn package provides a PCA class in the *decomposition* module, which will be used on the [iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# shape of X and y\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.92461621,  0.05301557,  0.01718514,  0.00518309])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate a PCA object with default values\n",
    "pca = PCA()\n",
    "\n",
    "# transform iris data\n",
    "pca_iris = pca.fit_transform(X)\n",
    "\n",
    "# check the variance explained by each dimension\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above output shows, 92.5% variance of the iris dataset can be explained by the first component or column.\n",
    "\n",
    "The PCA transformation is repeated using 2 dimensions only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca_iris = pca.fit_transform(X)\n",
    "\n",
    "# check the shape of the transformed dataset\n",
    "pca_iris.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from the shape of the PCA-transformed dataset that the 150 observations only have 2 columns now.\n",
    "\n",
    "We can easily check the total variance explained by the two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.978\n"
     ]
    }
   ],
   "source": [
    "print(\"{:.3f}\".format(pca.explained_variance_ratio_.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform knn classification using the pca-transfomed 2-component dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], 'weights': ['distance', 'uniform']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# values for tuning of knn paraemeters n_neighbors and weights \n",
    "n_neighbor_values = list(range(1,31))\n",
    "weight_values = ['distance', 'uniform']\n",
    "grid_values = dict(n_neighbors=n_neighbor_values, weights=weight_values)\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# grid-seach parameters with 10-fold cross-validations\n",
    "gridKnn = GridSearchCV(estimator=knn, param_grid=grid_values, scoring='accuracy', cv=10)\n",
    "gridKnn.fit(pca_iris,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'n_neighbors': 8, 'weights': 'distance'} with a mean score of 0.97\n"
     ]
    }
   ],
   "source": [
    "print(\"The best parameters are {} with a mean score of {:.2f}\".format(gridKnn.best_params_, gridKnn.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, a grid search on the full iris dataset using the knn estimator and the same grid search values returned '13' and 'uniform' as the optimal values for *n_neighbors* and *weights* hyper-parameters, respectively.  \n",
    "\n",
    "More importantly, the best score achieved by training with all the components was 0.98, or just 1% better!\n",
    "\n",
    "The knn estimator tuned by performing a cross-validatede grid-search of the hyper-parameter space of *n_neighbors* and *weights* can now be used to make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridKnn.predict([[1.5,2.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if the requirement was that the PCA must explain at least a certain percent of variance rather than have *n_components*? In this case, *n_components* must be set to the required percent  rather than the required number of components when instantiating the PCA object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explain at least 90% of variance\n",
    "pca = PCA(n_components=0.90)\n",
    "\n",
    "pca_iris_var = pca.fit_transform(X)\n",
    "pca_iris_var.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.925\n"
     ]
    }
   ],
   "source": [
    "# total variance explained\n",
    "print(\"{:.3f}\".format(pca.explained_variance_ratio_.sum()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
